---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>
## About Me

I am a **third-year PhD student** at the **School of Computer Science, The University of Sydney**, under the supervision of [Assoc. Prof. Chang Xu](http://changxu.xyz/) and [Assist. Prof. Daochang Liu](https://daochang.site/).  

Previously, I worked as a **Research Assistant** at the **Singapore University of Technology and Design**, where I was supervised by [Assoc. Prof. Ngai-Man Cheung](https://sites.google.com/site/mancheung0407/). I earned both my **Bachelor's and Master's degrees** from the **Hanoi University of Science and Technology**, guided by [A/Prof. Huynh Thi Thanh Binh](https://soict.hust.edu.vn/en/assoc-prof-huynh-thi-thanh-binh.html).  

My research focuses on **computer vision** and **fundamental concepts in generative artificial intelligence**, exploring cutting-edge methodologies to advance the field.  


<!-- I am a third-year PhD student at the School of Computer Science, The University of Sydney. I am lucky to be supervised by Assoc.Prof Chang Xu and Assist.Prof Daochang Liu.

Before that I was Research Assistant at Singapore University of Technology and Design under supervision of Asso.Prof Ngai-Man Cheung. I obtained Bachelor and Master degree from Hanoi Univerisity of Science and Technology supervised by A/Prof Huynh Thi Thanh Binh. My research delves deeply into computer vision and fundamental concepts in generative artificial intelligence. -->

<!-- I obtained a PhD degree from Peking University supervised by A/Prof [Tingting Jiang](http://www.vie.group/ttj) and Prof [Yizhou Wang](https://cfcs.pku.edu.cn/english/people/faculty/yizhouwang/index.htm), and a Bachelor of Engineering from Tongji University. 
 -->


<!-- My research interests lie in intelligent surgical skill assessment using computer vision and generative artificial intelligence.
 -->

<!-- My research seeks to answer two principal questions, i.e., ‚Äúwhat‚Äù is performed and ‚Äúhow well‚Äù it is performed, by observing human activities from visual or multi-modal data and using generative learning frameworks. This is pivotal to human-centric machine intelligence to comprehend, collaborate with, and amplify human capabilities in daily and professional lives.  -->

<!-- This stems from the notion that human behavior is inherently a generative act, conditioned on driving goals, environmental contexts, and prior experiences. Such an ‚Äúunderstanding-by-generating" paradigm enables unsupervised learning of human activities. I also feel excited about delving into human skill assessment by viewing skills as emergent attributes generated from repeated action operations. My goal is to understand skill acquisition through generative learning, by connecting the generative process of actions and the learning curve of skills from novice to expert.  -->

<!-- My research about generative surgical AI lies in intelligent analysis of surgeries to enhance patient care and surgical education. The short-term goal is to demystify the intricacies of surgical procedures, e.g., tool usage and event patterns, using videos from minimally invasive or robotic surgeries, and establish links with surgical skills and clinical outcomes. My previous works have shown promising results; AI-based skill assessment on in-vivo clinical gastrectomy surgeries demonstrated remarkable correlations with the expert consensus. The long-term vision is to bring intelligence into the multimodal data of surgeries in an enriched context beyond videos, including perioperative imaging, robotic kinematics, textual records, and environmental and physiological metrics, to offer holistic insights into surgical procedures. By utilizing specialized models, foundational models, and specialized-foundational models of machine learning, my research aims to advance surgical data science from concept to translation in the next generation of surgery.
 -->
<!-- My research is rooted in understanding human action and skills with computer vision and generative artificial intelligence, which also involves fundamental problems in generative diffusion models and applications in automatic surgical skill assessment.
 -->
<!-- My research interests lie in computer vision, artificial intelligence, and related topics in the medical domain, specifically including:
- Generative AI, Diffusion Models
- Surgical AI, Surgical Skill Assessment
- Video Understanding and Action Analysis -->

<!-- My research interest includes neural machine translation and computer vision. I have published more than 100 papers at the top international AI conferences with total <a href='https://scholar.google.com/citations?user=DhtAFkwAAAAJ'>google scholar citations <strong><span id='total_cit'>260000+</span></strong></a> (You can also use google scholar badge <a href='https://scholar.google.com/citations?user=DhtAFkwAAAAJ'><img src="https://img.shields.io/endpoint?url={{ url | url_encode }}&logo=Google%20Scholar&labelColor=f6f6f6&color=9cf&style=flat&label=citations"></a>).
 -->

<span class='anchor' id='news'></span>

# üî• News
- *2025.01*: One paper accepted to ICLR 2025
- *2024.11*: One paper accepted to TPAMI 
- *2024.5* : I will join Adobe Inc. as Research Scientist Intern
- *2023.9* : One paper accepted to NeurIPS 2023
- *2023.4* : One paper accepted to ICML 2023
<!-- *2024.02*: Two papers accepted to CVPR 2024. 
*2024.02*: I will serve as an Area Chair for MICCAI 2024.  -->

<span class='anchor' id='publications'></span>





<br>

# üìù Publications - Selected Publications


<!-- <div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICML 2024</div><img src='images/ICML2024.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Bridging Data Gaps in Diffusion Models with Adversarial Noise-Based Transfer Learning](https://openreview.net/pdf?id=PpBs2iL0jv) 
<!-- <strong><span class='show_paper_citations' data='ElujT6oAAAAJ:_kc_bZDykSQC'></span></strong>
 -->




<!-- <div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2024</div><img src='images/CVPR2024-2.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Residual Learning in Diffusion Models](https://openreview.net/pdf?id=EHX9Zg0zW8) 

Junyu Zhang, **Daochang Liu**, Eunbyung Park, Shichao Zhang, Chang Xu

*IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024*

</div>
</div> -->

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICLR 2025</div><img src='images/ICLR2025.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Representative Guidance: Diffusion Model Sampling with Consistency](https://openreview.net/forum?id=gWgaypDBs8) <strong><span class='show_paper_citations' data='ElujT6oAAAAJ:_kc_bZDykSQC'></span></strong>

**Anh-Dung Dinh**, Daochang Liu, Chang Xu

*International Conference on Learning Representations (ICLR), 2025*

[[Code]](https://github.com/dungdinhanh/prog-guided-diffusion)

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">NeurIPS 2023</div><img src='images/NeurIPS2023-1.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Rethinking Conditional Diffusion Sampling with Progressive Guidance](https://neurips.cc/virtual/2023/poster/70851) <strong><span class='show_paper_citations' data='ElujT6oAAAAJ:_kc_bZDykSQC'></span></strong>

**Anh-Dung Dinh**, Daochang Liu, Chang Xu

*Conference on Neural Information Processing Systems (NeurIPS), 2023*

[[Code]](https://github.com/dungdinhanh/prog-guided-diffusion)

</div>
</div>





<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICML 2023</div><img src='images/ICML2023.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[PixelAsParam: A Gradient View on Diffusion Sampling with Guidance](https://proceedings.mlr.press/v202/dinh23a.html) <strong><span class='show_paper_citations' data='ElujT6oAAAAJ:5nxA0vEk-isC'></span></strong>

**Anh-Dung Dinh**, Daochang Liu, Chang Xu

*International Conference on Machine Learning (ICML), 2023*

[[Code]](https://github.com/dungdinhanh/pxpguided-diffusion)

</div>
</div>



<div class='paper-box'><div class='paper-box-image'><div><div class="badge">TPAMI 2024</div><img src='images/TPAMI.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[DiffAct++: Diffusion Action Segmentation](https://www.computer.org/csdl/journal/tp/5555/01/10772006/22gA5Z07dyE) <strong><span class='show_paper_citations' data='ElujT6oAAAAJ:5nxA0vEk-isC'></span></strong>

Daochang Liu, Qiyue Li, **Anh-Dung Dinh**, Tingting Jiang, Mubarak Shah, Chang Xu

*IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2024*

[[Code]](https://github.com/Finspire13/DiffAct)

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICCV 2023</div><img src='images/ICCV2023-1.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

 [Diffusion Action Segmentation](https://arxiv.org/abs/2303.17959) <strong><span class='show_paper_citations' data='ElujT6oAAAAJ:hqOjcs7Dif8C'></span></strong>

Daochang Liu, Qiyue Li, **Anh-Dung Dinh**, Tingting Jiang, Mubarak Shah, Chang Xu

*International Conference on Computer Vision (ICCV), 2023*

[[Project]](https://finspire13.github.io/DiffAct-Project-Page/) [[Code]](https://github.com/Finspire13/DiffAct) [[Video]](https://youtu.be/AZ0vjscL1nc)


</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">KDD2023</div><img src='images/KDD2023.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

 [Learning to Schedule in Diffusion Probabilistic Models](https://dl.acm.org/doi/abs/10.1145/3580305.3599412) <strong><span class='show_paper_citations' data='ElujT6oAAAAJ:hqOjcs7Dif8C'></span></strong>

Yunke Wang, Xiyu Wang, **Anh-Dung Dinh**, Bo Du, Chang Xu

*29th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD), 2023*

<!-- [[Project]](https://finspire13.github.io/DiffAct-Project-Page/) [[Code]](https://github.com/Finspire13/DiffAct) [[Video]](https://youtu.be/AZ0vjscL1nc) -->


</div>
</div>





<br>

<!-- # üìù Publications - Computer Vision and Human Action Understanding 


 -->




<span class='anchor' id='teaching'></span>

# üìñ Teaching
- *2024-2025*, Teaching Assistant/Tutor for OCMP5329 Deep Learning, The University of Sydney
- *2024-2025*, Guest Lecturer for Summer course on Generative AI, The University of Sydney
- *2023-2025*, Invited lecturer/Tutor for COMP5329  Deep Learning, The University of Sydney


<span class='anchor' id='awards'></span>

# üéñ Awards
- *2022.08 - 2025.12*, Ph.D scholarships, Faculty of Engineering, The University of Sydney
- *2023.12*, Travel Grant, Conference on Neural Information Processing Systems
- *2019.08*, Master Scholarship, 100% tuition fee for excellent Engineer Students (top 10%) 
- *2017.05*, Research Scholarship, Student Research Conference, HUST (Second Runner)

<!-- <span class='anchor' id='talks'></span>

# üí¨ Talks
- *2023*, "Human Action and Skill Understanding in Long Videos", Beihang University
- *2023*, "What Does ChatGPT mean to AI Research", The University of Sydney
- *2023*, "Research in AI", The University of Sydney
- *2022*, "Biomedical Image Analysis", The University of Sydney
- *2021*, "Video-Based Surgical Skill Assessment - Improving Surgical Care by Computer Vision", Baidu Inc.
- *2020*, "Computer-Aided Surgical Skill Assessment", Microsoft Research Asia
- *2019*, "Surgical Skill Assessment on In-Vivo Clinical Data via the Clearness of Operating Field", MICCAI -->

<!-- <span class='anchor' id='grants'></span>

# üåü Grants

<!-- - *2023 - 2024*, Artificial Intelligence Empowered Surgical Skill Assessment (Lead CI), $7840 USD, Google Cloud Research Credits Award  -->

<!-- - *2024 Q1-Q2*, Large Language Models-Powered Automatic Annotation for Large-Scale Data (Cheif Investigator), 256000 GPU Hours (256K AUD Equivalent), The National Computational Infrastructure (NCI) AI Flagship Scheme, Australia

- *2024 Q1-Q2*, Automatically Assessing Surgical Skills in Enriched Context (Lead CI), 12000 GPU Hours (12K AUD Equivalent), Sydney Informatics Hub (SIH) HPC Allocation Scheme 2024 Round 1, Australia

- *2023 Q1-Q2*, Image Generation for the Colour Vision Impaired (Lead CI), 10000 GPU Hours (10K AUD Equivalent), The National Computational Infrastructure (NCI) Adapter Scheme, Australia 

<span class='anchor' id='services'></span> --> 

# ‚ô•Ô∏è Services

- *2024*, Publication Chair of The 13th International Symposium on Information And Communication Technology (SOICT 2024)
- Reviewer of TPAMI, TMM, ICLR (2023, 2024, 2025), ICML (2024, 2025), CVPR (2023, 2024, 2025), NeurIPS (2023, 2024), ICCV (2023), ECCV (2024), AAAI (2023, 2024, 2025)
<!-- # - -->

**Contact:** anh-dung.dinh@sydney.edu.au, anhdungdinh.vn21@gmail.com

**Last Update:** May 03, 2024

<!-- <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=EPHsmQIJLbnhIay_lL2JI0tJ1EPMrLTnAPwg8zuvHkY&cl=ffffff&w=300"></script>
<script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=uYMY2rGQRlj9jS9tWhBb3Q_cOVSqZYGQhXN06Yai_4U&cl=ffffff&w=a"></script> -->
<!-- <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=uYMY2rGQRlj9jS9tWhBb3Q_cOVSqZYGQhXN06Yai_4U&cl=ffffff&w=300"></script>

<script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=n6_VS48zC5grhWgVMqU4b1Uwrz-F7aRFEP6vHKiL9PM&cl=ffffff&w=a"></script> -->

<script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=n6_VS48zC5grhWgVMqU4b1Uwrz-F7aRFEP6vHKiL9PM&cl=ffffff&w=300"></script>